{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package in development mode (run once)\n",
    "# !pip install -e .\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for development\n",
    "src_dir = Path().cwd().parent / \"src\" if 'notebooks' in str(Path().cwd()) else Path().cwd() / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import our production modules\n",
    "from utils.config import ConfigManager\n",
    "from training.trainer import TrainingManager, HyperparameterSweepManager\n",
    "from data.datasets import MyanmarSatellite, Euro_0512, AI4Boundaries\n",
    "from utils.visualization import visualize_geotiff_samples, plot_training_curves\n",
    "from metrics.metrics import MetricsCalculator\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "print(\"‚úÖ Field Delineation Production Package Loaded Successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Management\n",
    "\n",
    "The new system uses YAML-based configuration management for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default configuration\n",
    "config = ConfigManager(\"../config/default_config.yaml\")\n",
    "\n",
    "# Display current configuration\n",
    "print(\"üìã Current Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show key configuration sections\n",
    "sections = ['model', 'training', 'data', 'losses']\n",
    "for section in sections:\n",
    "    section_config = config.get_section(section)\n",
    "    if section_config:\n",
    "        print(f\"\\n{section.upper()}:\")\n",
    "        for key, value in section_config.items():\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize configuration for this experiment\n",
    "experiment_config = {\n",
    "    'training': {\n",
    "        'max_epochs': 5,  # Short demo\n",
    "        'batch_size': 16,  # Smaller for demo\n",
    "        'learning_rate': 0.01,\n",
    "        'precision': '16-mixed'\n",
    "    },\n",
    "    'model': {\n",
    "        'arch': 'unetplusplus',\n",
    "        'encoder': 'resnet50',  # Lighter for demo\n",
    "        'in_channels': 9,\n",
    "        'out_channels': 3\n",
    "    },\n",
    "    'data': {\n",
    "        'dataset_type': 'MyanmarSatellite',\n",
    "        'channels': 9,\n",
    "        'augment': True,\n",
    "        'cache_data': False\n",
    "    },\n",
    "    'losses': {\n",
    "        'loss_1': 'DiceLoss',\n",
    "        'loss_2': 'ComboLoss', \n",
    "        'loss_3': 'BCELoss',\n",
    "        'weight_1': 0.8,\n",
    "        'weight_2': 0.0,\n",
    "        'weight_3': 0.8\n",
    "    },\n",
    "    'logging': {\n",
    "        'project': 'notebook-demo',\n",
    "        'use_wandb': False  # Disable for demo\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update configuration\n",
    "config.update(experiment_config)\n",
    "print(\"‚úÖ Configuration updated for notebook demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Let's explore the datasets available in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Update these paths to your actual data directories\n",
    "DATA_PATHS = {\n",
    "    'myanmar_train': '../Datasets/MyanmarAnnotations/Resolution_0_5/Masks/Cropped/Train/',\n",
    "    'myanmar_val': '../Datasets/MyanmarAnnotations/Resolution_0_5/Masks/Cropped/Val/',\n",
    "    'euro_data': '../Datasets/AI4Boundaries/shapefiles/Masks/',\n",
    "    'ai4b_train': '../Datasets/AI4Boundaries/ESRI/train/',\n",
    "}\n",
    "\n",
    "def check_data_availability():\n",
    "    \"\"\"Check which datasets are available.\"\"\"\n",
    "    available_datasets = []\n",
    "    \n",
    "    for name, path in DATA_PATHS.items():\n",
    "        if os.path.exists(path):\n",
    "            file_count = len(list(Path(path).glob(\"*.tif\")))\n",
    "            available_datasets.append((name, path, file_count))\n",
    "            print(f\"‚úÖ {name}: {file_count} files found\")\n",
    "        else:\n",
    "            print(f\"‚ùå {name}: Path not found - {path}\")\n",
    "    \n",
    "    return available_datasets\n",
    "\n",
    "available_data = check_data_availability()\n",
    "\n",
    "if not available_data:\n",
    "    print(\"\\n‚ö†Ô∏è No datasets found. Please update DATA_PATHS to point to your data directories.\")\n",
    "    print(\"For demo purposes, we'll create a synthetic dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small demo dataset if real data isn't available\n",
    "def create_demo_dataset():\n",
    "    \"\"\"Create synthetic data for demonstration.\"\"\"\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    class DemoDataset(Dataset):\n",
    "        def __init__(self, size=50):\n",
    "            self.size = size\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Create synthetic satellite imagery\n",
    "            image = torch.randn(9, 256, 256)  # 9 channels\n",
    "            \n",
    "            # Create synthetic masks (interior, border, distance)\n",
    "            mask = torch.zeros(3, 256, 256)\n",
    "            \n",
    "            # Random field shapes\n",
    "            center_x, center_y = np.random.randint(64, 192, 2)\n",
    "            size_x, size_y = np.random.randint(30, 80, 2)\n",
    "            \n",
    "            # Interior mask\n",
    "            mask[0, center_y-size_y//2:center_y+size_y//2, \n",
    "                    center_x-size_x//2:center_x+size_x//2] = 1\n",
    "            \n",
    "            # Border mask (simplified)\n",
    "            mask[1] = mask[0] * 0.5\n",
    "            \n",
    "            # Binary mask for loss masking\n",
    "            binary = torch.ones_like(mask)\n",
    "            \n",
    "            return {\n",
    "                'image': image.float(),\n",
    "                'mask': mask.float(),\n",
    "                'binary': binary.float(),\n",
    "                'file_path': f'demo_sample_{idx}.tif',\n",
    "                'mask_path': f'demo_mask_{idx}.tif'\n",
    "            }\n",
    "    \n",
    "    return DemoDataset\n",
    "\n",
    "# Use demo data if real data isn't available\n",
    "if not available_data:\n",
    "    DemoDataset = create_demo_dataset()\n",
    "    print(\"‚úÖ Demo dataset class created\")\n",
    "else:\n",
    "    print(\"‚úÖ Real datasets available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with TrainingManager\n",
    "\n",
    "The `TrainingManager` handles the entire training pipeline automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config with actual data paths if available\n",
    "if available_data:\n",
    "    # Use the first available dataset\n",
    "    if any('myanmar' in name for name, _, _ in available_data):\n",
    "        config.update({\n",
    "            'data': {\n",
    "                'dataset_type': 'MyanmarSatellite',\n",
    "                'train_dir': DATA_PATHS['myanmar_train'],\n",
    "                'val_dir': DATA_PATHS['myanmar_val'],\n",
    "                'test_dir': DATA_PATHS['myanmar_val'],\n",
    "                'channels': 9,\n",
    "                'augment': True\n",
    "            }\n",
    "        })\n",
    "    elif any('euro' in name for name, _, _ in available_data):\n",
    "        config.update({\n",
    "            'data': {\n",
    "                'dataset_type': 'Euro_0512',\n",
    "                'train_dir': DATA_PATHS['euro_data'],\n",
    "                'channels': 9,\n",
    "                'augment': True\n",
    "            }\n",
    "        })\n",
    "else:\n",
    "    # Use demo dataset\n",
    "    config.update({\n",
    "        'data': {\n",
    "            'dataset_type': 'Demo',  # Special flag for demo\n",
    "            'channels': 9,\n",
    "            'augment': True\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"üìä Using dataset: {config.get('data.dataset_type')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TrainingManager\n",
    "print(\"üöÄ Initializing Training Manager...\")\n",
    "\n",
    "trainer_manager = TrainingManager(config)\n",
    "\n",
    "# For demo dataset, we need to manually inject it\n",
    "if config.get('data.dataset_type') == 'Demo':\n",
    "    # Create demo datasets\n",
    "    train_dataset = DemoDataset(size=100)\n",
    "    val_dataset = DemoDataset(size=20)\n",
    "    test_dataset = DemoDataset(size=20)\n",
    "    \n",
    "    # Store in trainer manager\n",
    "    trainer_manager.datasets = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    }\n",
    "    \n",
    "    # Create model manually\n",
    "    trainer_manager.model = trainer_manager.create_model(train_dataset, val_dataset, test_dataset)\n",
    "    trainer_manager.trainer = trainer_manager.create_trainer()\n",
    "    \n",
    "    print(\"‚úÖ Demo datasets created and configured\")\n",
    "else:\n",
    "    # Setup with real data\n",
    "    model, trainer = trainer_manager.setup_training()\n",
    "    print(\"‚úÖ Real datasets loaded and configured\")\n",
    "\n",
    "print(f\"Training samples: {len(trainer_manager.datasets['train'])}\")\n",
    "print(f\"Validation samples: {len(trainer_manager.datasets['val'])}\")\n",
    "print(f\"Test samples: {len(trainer_manager.datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"üéØ Starting Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    results = trainer_manager.train()\n",
    "    \n",
    "    print(\"\\nüéâ Training Completed Successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training time: {results['training_time']:.2f} seconds\")\n",
    "    print(f\"Final epoch: {results['current_epoch']}\")\n",
    "    print(f\"Total steps: {results['global_step']}\")\n",
    "    \n",
    "    if results['test_results']:\n",
    "        print(\"\\nüìä Test Results:\")\n",
    "        for i, result in enumerate(results['test_results']):\n",
    "            print(f\"  Run {i+1}: {result}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"This might be due to missing data or environment issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization function for our demo\n",
    "def visualize_demo_results(model, dataset, num_samples=5):\n",
    "    \"\"\"Visualize model predictions on demo data.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get sample\n",
    "        sample = dataset[i]\n",
    "        image = sample['image'].unsqueeze(0)\n",
    "        mask = sample['mask'].unsqueeze(0)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'predict'):\n",
    "                pred = model.predict(image)\n",
    "            else:\n",
    "                pred = torch.sigmoid(model(image))\n",
    "        \n",
    "        # Convert to numpy for plotting\n",
    "        image_np = image[0, :3].permute(1, 2, 0).cpu().numpy()\n",
    "        mask_np = mask[0, 0].cpu().numpy()\n",
    "        pred_np = pred[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Normalize image for display\n",
    "        image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(image_np)\n",
    "        axes[i, 0].set_title('Input (RGB)')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask_np, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_np, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Difference\n",
    "        diff = np.abs(pred_np - mask_np)\n",
    "        axes[i, 3].imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[i, 3].set_title('Absolute Error')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results if we have a trained model\n",
    "if trainer_manager.model is not None:\n",
    "    print(\"üñºÔ∏è Visualizing Model Predictions\")\n",
    "    try:\n",
    "        visualize_demo_results(trainer_manager.model, trainer_manager.datasets['val'], num_samples=3)\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "else:\n",
    "    print(\"No trained model available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics\n",
    "if trainer_manager.model is not None:\n",
    "    print(\"üìä Calculating Detailed Metrics\")\n",
    "    \n",
    "    # Create metrics calculator\n",
    "    metrics_calc = MetricsCalculator(channels=3, threshold=0.5)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    trainer_manager.model.eval()\n",
    "    val_dataset = trainer_manager.datasets['val']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(len(val_dataset), 20)):  # Evaluate on subset\n",
    "            sample = val_dataset[i]\n",
    "            image = sample['image'].unsqueeze(0)\n",
    "            mask = sample['mask'].unsqueeze(0)\n",
    "            \n",
    "            if hasattr(trainer_manager.model, 'predict'):\n",
    "                pred = trainer_manager.model.predict(image)\n",
    "            else:\n",
    "                pred = torch.sigmoid(trainer_manager.model(image))\n",
    "            \n",
    "            metrics_calc.update(pred, mask)\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_metrics = metrics_calc.compute()\n",
    "    \n",
    "    print(\"\\nüìà Validation Metrics:\")\n",
    "    print(\"=\" * 30)\n",
    "    for metric_name, value in final_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"No trained model available for metrics calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration-Based Experiments\n",
    "\n",
    "Show how easy it is to run different experiments with configuration changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different loss function configurations\n",
    "loss_experiments = {\n",
    "    'dice_only': {\n",
    "        'losses': {\n",
    "            'loss_1': 'DiceLoss',\n",
    "            'loss_2': 'ComboLoss',\n",
    "            'loss_3': 'BCELoss',\n",
    "            'weight_1': 1.0,\n",
    "            'weight_2': 0.0,\n",
    "            'weight_3': 0.0\n",
    "        }\n",
    "    },\n",
    "    'focal_tversky': {\n",
    "        'losses': {\n",
    "            'loss_1': 'FocalTverskyLoss',\n",
    "            'loss_2': 'ComboLoss',\n",
    "            'loss_3': 'BCELoss',\n",
    "            'weight_1': 0.8,\n",
    "            'weight_2': 0.0,\n",
    "            'weight_3': 0.2\n",
    "        }\n",
    "    },\n",
    "    'boundary_aware': {\n",
    "        'losses': {\n",
    "            'loss_1': 'DiceLoss',\n",
    "            'loss_2': 'BoundaryLoss',\n",
    "            'loss_3': 'FocalLoss',\n",
    "            'weight_1': 0.6,\n",
    "            'weight_2': 0.2,\n",
    "            'weight_3': 0.2,\n",
    "            'weight_4': 0.3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üß™ Available Loss Function Experiments:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for exp_name, exp_config in loss_experiments.items():\n",
    "    print(f\"\\n{exp_name.upper()}:\")\n",
    "    for key, value in exp_config['losses'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüí° To run an experiment, update config and create new TrainingManager:\")\n",
    "print(\"config.update(loss_experiments['dice_only'])\")\n",
    "print(\"trainer = TrainingManager(config)\")\n",
    "print(\"results = trainer.train()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Optimization Preview\n",
    "\n",
    "Demonstrate how hyperparameter sweeps can be configured (without running full sweep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to set up hyperparameter sweeps\n",
    "from training.trainer import create_default_sweep_config\n",
    "\n",
    "# Create sweep configuration\n",
    "sweep_config = create_default_sweep_config()\n",
    "\n",
    "print(\"üî¨ Hyperparameter Sweep Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Method: {sweep_config['method']}\")\n",
    "print(f\"Metric: {sweep_config['metric']}\")\n",
    "print(\"\\nParameters to optimize:\")\n",
    "\n",
    "for param, config in sweep_config['parameters'].items():\n",
    "    if 'values' in config:\n",
    "        print(f\"  {param}: {config['values'][:3]}...\" if len(config['values']) > 3 else f\"  {param}: {config['values']}\")\n",
    "    elif 'distribution' in config:\n",
    "        print(f\"  {param}: {config['distribution']} ({config.get('min', '')} - {config.get('max', '')})\")\n",
    "\n",
    "print(\"\\nüöÄ To run a full sweep:\")\n",
    "print(\"python scripts/sweep.py --sweep-type default --count 50 --project my-sweep\")\n",
    "print(\"\\nüìä Or from Python:\")\n",
    "print(\"sweep_manager = HyperparameterSweepManager(base_config, sweep_config)\")\n",
    "print(\"sweep_manager.run_sweep(count=50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Inference Pipeline\n",
    "\n",
    "Demonstrate how to use trained models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference pipeline example\n",
    "def run_inference_pipeline(model, test_sample, threshold=0.5):\n",
    "    \"\"\"Run complete inference pipeline on a test sample.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    image = test_sample['image'].unsqueeze(0)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, 'predict'):\n",
    "            predictions = model.predict(image)\n",
    "        else:\n",
    "            logits = model(image)\n",
    "            predictions = torch.sigmoid(logits)\n",
    "    \n",
    "    # Post-process predictions\n",
    "    binary_predictions = (predictions > threshold).float()\n",
    "    \n",
    "    # Calculate confidence scores\n",
    "    confidence_scores = {\n",
    "        'mean_confidence': predictions.mean().item(),\n",
    "        'max_confidence': predictions.max().item(),\n",
    "        'min_confidence': predictions.min().item(),\n",
    "        'std_confidence': predictions.std().item()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'binary_predictions': binary_predictions,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'input_shape': image.shape\n",
    "    }\n",
    "\n",
    "# Run inference example\n",
    "if trainer_manager.model is not None and trainer_manager.datasets:\n",
    "    print(\"üéØ Running Inference Pipeline\")\n",
    "    \n",
    "    # Get test sample\n",
    "    test_sample = trainer_manager.datasets['test'][0]\n",
    "    \n",
    "    # Run inference\n",
    "    inference_results = run_inference_pipeline(trainer_manager.model, test_sample)\n",
    "    \n",
    "    print(\"\\nüìä Inference Results:\")\n",
    "    print(\"=\" * 25)\n",
    "    print(f\"Input shape: {inference_results['input_shape']}\")\n",
    "    print(f\"Prediction shape: {inference_results['predictions'].shape}\")\n",
    "    \n",
    "    print(\"\\nüéØ Confidence Scores:\")\n",
    "    for metric, value in inference_results['confidence_scores'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize inference result\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    # Input\n",
    "    input_vis = test_sample['image'][:3].permute(1, 2, 0).numpy()\n",
    "    input_vis = (input_vis - input_vis.min()) / (input_vis.max() - input_vis.min())\n",
    "    axes[0].imshow(input_vis)\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    pred_vis = inference_results['predictions'][0, 0].cpu().numpy()\n",
    "    axes[1].imshow(pred_vis, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Prediction Probability')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Binary prediction\n",
    "    binary_vis = inference_results['binary_predictions'][0, 0].cpu().numpy()\n",
    "    axes[2].imshow(binary_vis, cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2].set_title('Binary Prediction')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No trained model available for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Deployment Tips\n",
    "\n",
    "Key points for moving from notebook to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment checklist\n",
    "deployment_checklist = {\n",
    "    \"Package Installation\": [\n",
    "        \"pip install -e .\",\n",
    "        \"Verify all dependencies are installed\",\n",
    "        \"Test import statements\"\n",
    "    ],\n",
    "    \"Configuration Management\": [\n",
    "        \"Create production config files\",\n",
    "        \"Set appropriate data paths\",\n",
    "        \"Configure logging and monitoring\"\n",
    "    ],\n",
    "    \"Training Pipeline\": [\n",
    "        \"python scripts/train.py --config production_config.yaml\",\n",
    "        \"Monitor training with wandb/tensorboard\",\n",
    "        \"Save and version model checkpoints\"\n",
    "    ],\n",
    "    \"Inference Pipeline\": [\n",
    "        \"python scripts/inference.py --checkpoint best_model.ckpt --data-dir ./test\",\n",
    "        \"Implement batch processing for large datasets\",\n",
    "        \"Add error handling and logging\"\n",
    "    ],\n",
    "    \"Hyperparameter Optimization\": [\n",
    "        \"python scripts/sweep.py --sweep-type architecture --count 100\",\n",
    "        \"Define custom sweep configurations\",\n",
    "        \"Monitor and analyze sweep results\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üöÄ Production Deployment Checklist\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\nüìã {category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚úÖ {item}\")\n",
    "\n",
    "print(\"\\nüéØ Key Benefits of Production Pipeline:\")\n",
    "benefits = [\n",
    "    \"Reproducible experiments with configuration management\",\n",
    "    \"Automated training orchestration with callbacks\",\n",
    "    \"Easy hyperparameter optimization with wandb\",\n",
    "    \"Comprehensive CLI interface for all operations\", \n",
    "    \"Production-ready inference pipeline\",\n",
    "    \"Modular code structure for easy maintenance\",\n",
    "    \"Built-in visualization and analysis tools\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(f\"  üåü {benefit}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Satellite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
